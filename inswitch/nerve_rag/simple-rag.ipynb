{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-based RAG system for Nerve API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from typing import Callable, List, Tuple, Union\n",
    "import sys\n",
    "\n",
    "# Add path\n",
    "sys.path.append('../..')\n",
    "\n",
    "from inswitch.llm.model import get_openai_model_config\n",
    "from inswitch.agent.basic import get_llm_agent, get_fixed_reply_agent\n",
    "from inswitch.agent.ragagent import RagAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all relevant Nerve API documents to be added to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"https://docs.nerve.cloud/developer_guide/dna/\"\n",
    "doc_2 = \"https://docs.nerve.cloud/developer_guide/networking/\"\n",
    "doc_3 = \"https://docs.nerve.cloud/developer_guide/codesys/\"\n",
    "doc_4 = \"https://docs.nerve.cloud/developer_guide/ms-api/api-pointers/\"\n",
    "doc_5 = \"https://docs.nerve.cloud/developer_guide/ms-api/api-examples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-enhanced agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerve_rag_agent = RagAgent(\n",
    "    name = \"rag_agent\", \n",
    "    docs_path = [doc_1, doc_2, doc_3, doc_4, doc_5],\n",
    "    max_internal_turns=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mmoderator\u001b[0m (to deployment_plan_announcer):\n",
      "\n",
      "What should be deployed? on which machine?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mdeployment_plan_announcer\u001b[0m (to moderator):\n",
      "\n",
      "Deploy the following workloads for Machine: M00001 (Type: MTC)\n",
      "\n",
      "      - Workload: ngix, Version: ngix_27 (1.27.2)\n",
      "      - Workload: nodejs, Version: nodejs_23 (23.2.0)\n",
      "      - Workload: alarm_record\n",
      "      - Workload: mqtt_sender\n",
      "      \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mmoderator\u001b[0m (to rag_agent):\n",
      "\n",
      "Provide context\n",
      "Context: \n",
      "Deploy the following workloads for Machine: M00001 (Type: MTC)\n",
      "\n",
      "      - Workload: ngix, Version: ngix_27 (1.27.2)\n",
      "      - Workload: nodejs, Version: nodejs_23 (23.2.0)\n",
      "      - Workload: alarm_record\n",
      "      - Workload: mqtt_sender\n",
      "      \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "moderator = get_fixed_reply_agent(\n",
    "    name=\"moderator\",\n",
    "    reply = \"\"\n",
    ")\n",
    "\n",
    "deploy_plan = \"\"\"Deploy the following workloads for Machine: M00001 (Type: MTC)\n",
    "\n",
    "      - Workload: ngix, Version: ngix_27 (1.27.2)\n",
    "      - Workload: nodejs, Version: nodejs_23 (23.2.0)\n",
    "      - Workload: alarm_record\n",
    "      - Workload: mqtt_sender\n",
    "      \n",
    "\"\"\"\n",
    "\n",
    "deployment_plan_announcer = get_fixed_reply_agent(\n",
    "    'deployment_plan_announcer',\n",
    "    reply=deploy_plan\n",
    ")\n",
    "\n",
    "chat_result = moderator.initiate_chats(\n",
    "    [\n",
    "        {\n",
    "            \"recipient\": deployment_plan_announcer,\n",
    "            \"message\": \"What should be deployed? on which machine?\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": nerve_rag_agent,\n",
    "            \"message\": \"Provide context\",\n",
    "            \"max_turns\": 1,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
